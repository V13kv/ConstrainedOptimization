{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_4lLcnpLGKp"
   },
   "source": [
    "# Working RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0_0C_uL82Sx",
    "outputId": "71742178-10a9-4782-95ae-013ed0bae164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: x = 2.3675, y = 1.6903, Loss = 0.8959\n",
      "Iteration 20: x = 2.4194, y = 1.7414, Loss = 0.7412\n",
      "Iteration 30: x = 2.4928, y = 1.8102, Loss = 0.5505\n",
      "Iteration 40: x = 2.5798, y = 1.8837, Loss = 0.3667\n",
      "Iteration 50: x = 2.6723, y = 1.9484, Loss = 0.2174\n",
      "Iteration 60: x = 2.7625, y = 1.9933, Loss = 0.1128\n",
      "Iteration 70: x = 2.8432, y = 2.0144, Loss = 0.0494\n",
      "Iteration 80: x = 2.9089, y = 2.0161, Loss = 0.0168\n",
      "Iteration 90: x = 2.9568, y = 2.0083, Loss = 0.0038\n",
      "Iteration 100: x = 2.9872, y = 2.0005, Loss = 0.0003\n",
      "\n",
      "Optimized parameters:\n",
      "x = 2.9893, y = 1.9999, z = 2.9893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# https://arxiv.org/pdf/1908.03265\n",
    "# https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for\n",
    "class RAdam:\n",
    "    \"\"\"Rectified Adam (RAdam) optimizer.\n",
    "\n",
    "    This optimizer dynamically adjusts the learning rate using a rectification term\n",
    "    to stabilize the variance of the adaptive learning rate, especially in early training.\n",
    "\n",
    "    Attributes:\n",
    "        lr (float): Learning rate.\n",
    "        betas (tuple): Coefficients for computing running averages of gradient and its square.\n",
    "        eps (float): Term added to the denominator to improve numerical stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        \"\"\"Initialize the RAdam optimizer.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate (default: 1e-3).\n",
    "            betas (tuple): Coefficients for computing running averages of gradient and its square\n",
    "                           (default: (0.9, 0.999)).\n",
    "            eps (float): Term added to the denominator to improve numerical stability (default: 1e-8).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `lr`, `betas`, or `eps` are invalid.\n",
    "        \"\"\"\n",
    "        # Parameter validation\n",
    "        if lr <= 0:\n",
    "            raise ValueError(\"Learning rate (lr) must be positive.\")\n",
    "        if not (0.0 <= betas[0] < 1.0 and 0.0 <= betas[1] < 1.0):\n",
    "            raise ValueError(\"Betas must be in the range [0, 1).\")\n",
    "        if eps <= 0:\n",
    "            raise ValueError(\"Epsilon (eps) must be positive.\")\n",
    "\n",
    "        # Parameter initialization\n",
    "        self.lr = lr    # Learning rate\n",
    "        self.betas = betas  # Coefficients for moment estimates\n",
    "        self.eps = eps  # Small constant for numerical stability\n",
    "\n",
    "        # Initialize moment estimates (first and second moments)\n",
    "        self.m = None  # First moment estimate (mean of gradients)\n",
    "        self.v = None  # Second moment estimate (uncentered variance of gradients)\n",
    "\n",
    "        # Asymptotic upper bound for the rectification term\n",
    "        self.rho_inf = 2 / (1 - betas[1]) - 1\n",
    "\n",
    "    def solve(self, objective_function, params, gradient_function, num_iterations=100):\n",
    "        \"\"\"Solve the optimization problem using RAdam optimizer.\n",
    "\n",
    "        Args:\n",
    "            objective_function (callable): The objective function to minimize.\n",
    "            params (list of np.ndarray): List of parameters to optimize.\n",
    "            gradient_function (callable): Function to compute gradients of the objective function.\n",
    "            num_iterations (int): Number of optimization iterations (default: 100).\n",
    "\n",
    "        Returns:\n",
    "            list of np.ndarray: Optimized parameters.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If `objective_function`, `gradient_function` are invalid.\n",
    "            ValueError: If `num_iterations` is invalid.\n",
    "        \"\"\"\n",
    "        # Validate objective_function\n",
    "        if not callable(objective_function):\n",
    "            raise TypeError(\"objective_function must be a callable function.\")\n",
    "\n",
    "        # Validate gradient_function\n",
    "        if not callable(gradient_function):\n",
    "            raise TypeError(\"gradient_function must be a callable function.\")\n",
    "\n",
    "        # Validate num_iterations\n",
    "        if not isinstance(num_iterations, int) or num_iterations <= 0:\n",
    "            raise ValueError(\"num_iterations must be a positive integer.\")\n",
    "\n",
    "        # Initialize moment estimates\n",
    "        self._initialize_moments(params)\n",
    "\n",
    "        # Optimization loop\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            # Compute gradients\n",
    "            grads = list(gradient_function(*params))\n",
    "\n",
    "            # Perform optimization step\n",
    "            params = self.step(params, grads, iteration)\n",
    "\n",
    "            # Print progress\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                loss = objective_function(*params)\n",
    "                print(f\"Iteration {iteration + 1}: x = {params[0][0]:.4f}, y = {params[1][0]:.4f}, Loss = {loss[0]:.4f}\")\n",
    "\n",
    "        return params\n",
    "\n",
    "    def step(self, params, grads, t):\n",
    "        \"\"\"Perform a single optimization step using RAdam.\n",
    "\n",
    "        Args:\n",
    "            params (list of np.ndarray): Current parameters.\n",
    "            grads (list of np.ndarray): Gradients of the objective function with respect to the parameters.\n",
    "            t (int): Current iteration number (used for bias correction).\n",
    "\n",
    "        Returns:\n",
    "            list of np.ndarray: Updated parameters.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If parameter and gradient shapes do not match.\n",
    "        \"\"\"\n",
    "        # Ensure we have gradient for each parameter (shapes are equal)\n",
    "        assert len(params) == len(grads), \"Parameter and gradient shapes must match.\"\n",
    "\n",
    "        # Lazy initialization of moment estimates\n",
    "        if self.m is None or self.v is None:\n",
    "            self._initialize_moments(params)\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            # Ensure gradients and parameters have the same shape (needed for multidimensional parameters such as matricies)\n",
    "            assert param.shape == grad.shape, \"Parameter and gradient shapes must match.\"\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            self.m[i] = self.betas[0] * self.m[i] + (1 - self.betas[0]) * grad\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v[i] = self.betas[1] * self.v[i] + (1 - self.betas[1]) * (grad ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.betas[0] ** t)\n",
    "\n",
    "            # Rectification term\n",
    "            rho_t = self.rho_inf - 2 * t * (self.betas[1] ** t) / (1 - self.betas[1] ** t)\n",
    "\n",
    "            if rho_t > 4:\n",
    "                # Rectified update\n",
    "                l_t = np.sqrt( (1 - self.betas[1]**t) / (self.v[i] + self.eps) )\n",
    "                r_t = np.sqrt(\n",
    "                    ((rho_t - 4) * (rho_t - 2) * self.rho_inf) / ((self.rho_inf - 4) * (self.rho_inf - 2) * rho_t)\n",
    "                )\n",
    "                param_update = r_t * m_hat * l_t\n",
    "            else:\n",
    "                # Unrectified update (similar to Adam)\n",
    "                param_update = m_hat\n",
    "\n",
    "            # Update parameter\n",
    "            param -= self.lr * param_update\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _initialize_moments(self, params):\n",
    "        \"\"\"\n",
    "        Initialize moment estimates (self.m and self.v) to zero vectors.\n",
    "\n",
    "        Args:\n",
    "            params (list of np.ndarray): Current parameters.\n",
    "        \"\"\"\n",
    "        self.m = [np.zeros_like(p) for p in params]  # First moment estimate\n",
    "        self.v = [np.zeros_like(p) for p in params]  # Second moment estimate\n",
    "\n",
    "# Example usage with an arbitrary objective function\n",
    "if __name__ == \"__main__\":\n",
    "    # Define an example objective function (e.g., f(x, y) = x^2 + y^2)\n",
    "    def objective_function(x, y, z):\n",
    "        return (x-3)**2 + (y-2)**2 + (z-3)**2\n",
    "\n",
    "    # Define the gradient of the objective function\n",
    "    def gradient(x, y, z):\n",
    "        return 2 * (x-3), 2 * (y-2), 2*(z-3)\n",
    "\n",
    "    # Initialize parameters (e.g., x and y)\n",
    "    params = [np.array([1.0]), np.array([1.0]), np.array([1.0])]  # Example: x = 1.0, y = 2.0\n",
    "\n",
    "    # Initialize RAdam optimizer\n",
    "    optimizer = RAdam(lr=0.1)\n",
    "    result = optimizer.solve(objective_function, params, gradient)\n",
    "\n",
    "    # Final optimized parameters\n",
    "    print(\"\\nOptimized parameters:\")\n",
    "    print(f\"x = {result[0][0]:.4f}, y = {result[1][0]:.4f}, z = {result[2][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSakyQVRLWQ6"
   },
   "source": [
    "## Pytorch's RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkfH8PNY9Ofy",
    "outputId": "085b2dc3-3aa2-4789-88fe-50e01cead068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: x = 1.8125, y = 1.8125, Loss = 0.2270\n",
      "Iteration 20: x = 1.8553, y = 1.8553, Loss = 0.1674\n",
      "Iteration 30: x = 1.9064, y = 1.9064, Loss = 0.1061\n",
      "Iteration 40: x = 1.9555, y = 1.9555, Loss = 0.0578\n",
      "Iteration 50: x = 1.9923, y = 1.9923, Loss = 0.0273\n",
      "Iteration 60: x = 2.0110, y = 2.0110, Loss = 0.0110\n",
      "Iteration 70: x = 2.0132, y = 2.0132, Loss = 0.0034\n",
      "Iteration 80: x = 2.0067, y = 2.0067, Loss = 0.0005\n",
      "Iteration 90: x = 1.9999, y = 1.9999, Loss = 0.0000\n",
      "Iteration 100: x = 1.9974, y = 1.9974, Loss = 0.0001\n",
      "\n",
      "Optimized parameters:\n",
      "x = 1.9974, y = 1.9974, z = 3.0078\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define an example objective function (e.g., f(x, y) = x^2 + y^2)\n",
    "def objective_function(x, y):\n",
    "    return (x-2)**2 + (y-2)**2 + (z-3)**2\n",
    "\n",
    "# Initialize parameters (e.g., x and y)\n",
    "x = torch.tensor([1.0], requires_grad=True)  # Example: x = 1.0\n",
    "y = torch.tensor([1.0], requires_grad=True)  # Example: y = 2.0\n",
    "z = torch.tensor([1.0], requires_grad=True)  # Example: y = 2.0\n",
    "\n",
    "# Initialize RAdam optimizer\n",
    "optimizer = optim.RAdam([x, y, z], lr=0.1)\n",
    "\n",
    "# Optimization loop\n",
    "num_iterations = 100\n",
    "for iteration in range(num_iterations):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the objective function\n",
    "    loss = objective_function(x, y)\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        print(f\"Iteration {iteration + 1}: x = {x.item():.4f}, y = {y.item():.4f}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Final optimized parameters\n",
    "print(\"\\nOptimized parameters:\")\n",
    "print(f\"x = {x.item():.4f}, y = {y.item():.4f}, z = {z.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvPspnKQLYgM"
   },
   "source": [
    "## Barrier function construction (SYMPY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCs4XU6mL8Dd"
   },
   "source": [
    "### First step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bkILNyxLcqW",
    "outputId": "d257531f-fd53-4f3c-e1a3-65dda9870002"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mu \\left(- \\log{\\left(- x_{1} - x_{2} + 1 \\right)} - \\log{\\left(x_{1} - x_{2} + 1 \\right)}\\right) + x_{1}^{2} + x_{2}^{2}$"
      ],
      "text/plain": [
       "mu*(-log(-x1 - x2 + 1) - log(x1 - x2 + 1)) + x1**2 + x2**2"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbolic variables\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "mu = sp.symbols('mu', positive=True)\n",
    "\n",
    "# Define the original objective function f(x)\n",
    "f = x1**2 + x2**2  # Example objective function\n",
    "\n",
    "# Define the non-linear inequality constraints g_i(x) <= 0\n",
    "g1 = x1 + x2 - 1  # Example constraint 1\n",
    "g2 = -x1 + x2 - 1  # Example constraint 2\n",
    "\n",
    "# Define the barrier function phi(x)\n",
    "phi = -sp.log(-g1) - sp.log(-g2)\n",
    "\n",
    "# Define the new objective function F(x, mu) with the barrier method applied\n",
    "F = f + mu * phi\n",
    "\n",
    "# Display the new objective function\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0T9gAK3MAPM"
   },
   "source": [
    "### Second step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKQA49asL6gY",
    "outputId": "2472497f-0d07-482c-d6b4-736fd10297da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Objective Function:\n",
      "x1**2 + x2**2\n",
      "\n",
      "Constraints:\n",
      "g1(x) = x1 + x2 - 1 <= 0\n",
      "g2(x) = -x1 + x2 - 1 <= 0\n",
      "\n",
      "New Objective Function with Barrier Method Applied:\n",
      "mu*(-log(-x1 - x2 + 1) - log(x1 - x2 + 1)) + x1**2 + x2**2\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "def create_objective_with_barrier(f, constraints, mu):\n",
    "    \"\"\"\n",
    "    Creates a new objective function with the barrier method applied.\n",
    "\n",
    "    Parameters:\n",
    "        f (sympy expression): The original objective function.\n",
    "        constraints (list of sympy expressions): List of non-linear inequality constraints (g_i(x) <= 0).\n",
    "        mu (sympy symbol or float): Barrier parameter.\n",
    "\n",
    "    Returns:\n",
    "        sympy expression: New objective function with the barrier method applied.\n",
    "    \"\"\"\n",
    "    # Define the barrier function phi(x)\n",
    "    phi = -sum(sp.log(-g) for g in constraints)\n",
    "\n",
    "    # Create the new objective function F(x, mu) = f(x) + mu * phi(x)\n",
    "    F = f + mu * phi\n",
    "\n",
    "    return F\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define symbolic variables\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "    mu = sp.symbols('mu', positive=True)  # Barrier parameter\n",
    "\n",
    "    # User-defined objective function\n",
    "    f = x1**2 + x2**2  # Example: f(x1, x2) = x1^2 + x2^2\n",
    "\n",
    "    # User-defined inequality constraints (g_i(x) <= 0)\n",
    "    constraints = [\n",
    "        x1 + x2 - 1,  # g1(x1, x2) = x1 + x2 - 1 <= 0\n",
    "        -x1 + x2 - 1  # g2(x1, x2) = -x1 + x2 - 1 <= 0\n",
    "    ]\n",
    "\n",
    "    # Create the new objective function with the barrier method applied\n",
    "    F = create_objective_with_barrier(f, constraints, mu)\n",
    "\n",
    "    # Display the new objective function\n",
    "    print(\"Original Objective Function:\")\n",
    "    print(f)\n",
    "    print(\"\\nConstraints:\")\n",
    "    for i, g in enumerate(constraints):\n",
    "        print(f\"g{i+1}(x) = {g} <= 0\")\n",
    "    print(\"\\nNew Objective Function with Barrier Method Applied:\")\n",
    "    print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh-VoOuTNANy"
   },
   "source": [
    "### Adding gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJv8IxkyNAFT",
    "outputId": "903fca15-9f42-4bd4-d1e4-65566062bb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the New Objective Function:\n",
      "∂F/∂x1 = mu*(-1/(x1 - x2 + 1) + 1/(-x1 - x2 + 1)) + 2*x1\n",
      "∂F/∂x2 = mu*(1/(x1 - x2 + 1) + 1/(-x1 - x2 + 1)) + 2*x2\n"
     ]
    }
   ],
   "source": [
    "def gradient_function(F, variables):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the function F with respect to the given variables.\n",
    "\n",
    "    Parameters:\n",
    "        F (sympy expression): The function to compute the gradient for.\n",
    "        variables (list of sympy symbols): List of variables to differentiate with respect to.\n",
    "\n",
    "    Returns:\n",
    "        list of sympy expressions: The gradient of F.\n",
    "    \"\"\"\n",
    "    gradient = [sp.diff(F, var) for var in variables]\n",
    "    return gradient\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Calculate the gradient of F with respect to x1 and x2\n",
    "    variables = [x1, x2]\n",
    "    gradient = gradient_function(F, variables)\n",
    "\n",
    "    # Display the gradient\n",
    "    print(\"Gradient of the New Objective Function:\")\n",
    "    for i, grad in enumerate(gradient):\n",
    "        print(f\"∂F/∂{variables[i]} = {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_PgHngoP9T3"
   },
   "source": [
    "## Solve constrained problem (inequality constraints only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6P2bKxeQDHV",
    "outputId": "b8cbd9ee-f1c7-4333-dce1-a21b35cb678c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.]\n",
      "0.01 [-2.66666667]\n",
      "[-1.]\n",
      "0.01 [-0.66666667]\n",
      "[-0.97333333]\n",
      "0.01 [-2.62630535]\n",
      "[-0.99333333]\n",
      "0.01 [-0.66799782]\n",
      "[-0.94707028]\n",
      "0.01 [-2.5853074]\n",
      "[-0.98665336]\n",
      "0.01 [-0.6689281]\n",
      "[-0.92121721]\n",
      "0.01 [-2.54371178]\n",
      "[-0.97996407]\n",
      "0.01 [-0.66945152]\n",
      "[-0.89578009]\n",
      "0.01 [-0.01719123]\n",
      "[-0.97326956]\n",
      "0.01 [-0.01731551]\n",
      "[-0.89560818]\n",
      "0.01 [-0.02560114]\n",
      "[-0.9730964]\n",
      "0.01 [-0.02582601]\n",
      "[-0.89535216]\n",
      "0.01 [-0.0324269]\n",
      "[-0.97283814]\n",
      "0.01 [-0.03274267]\n",
      "[-0.8950279]\n",
      "0.01 [-0.03834316]\n",
      "[-0.97251072]\n",
      "0.01 [-0.03873868]\n",
      "[-0.89464446]\n",
      "0.01 [-0.04363473]\n",
      "[-0.97212333]\n",
      "0.01 [-0.04409881]\n",
      "Iteration 10: x = -0.8942, y = -0.9717, Loss = 0.6163\n",
      "[-0.89420812]\n",
      "0.01 [-0.0484601]\n",
      "[-0.97168234]\n",
      "0.01 [-0.04898192]\n",
      "[-0.89372352]\n",
      "0.01 [-0.05291974]\n",
      "[-0.97119252]\n",
      "0.01 [-0.05348896]\n",
      "[-0.89319432]\n",
      "0.01 [-0.05708253]\n",
      "[-0.97065763]\n",
      "0.01 [-0.05768918]\n",
      "[-0.89262349]\n",
      "0.01 [-0.06099819]\n",
      "[-0.97008074]\n",
      "0.01 [-0.06163262]\n",
      "[-0.89201351]\n",
      "0.01 [-0.06470401]\n",
      "[-0.96946442]\n",
      "0.01 [-0.06535679]\n",
      "[-0.89136647]\n",
      "0.01 [-0.06822878]\n",
      "[-0.96881085]\n",
      "0.01 [-0.06889062]\n",
      "[-0.89068418]\n",
      "0.01 [-0.07159526]\n",
      "[-0.96812194]\n",
      "0.01 [-0.07225699]\n",
      "[-0.88996823]\n",
      "0.01 [-0.07482181]\n",
      "[-0.96739937]\n",
      "0.01 [-0.07547427]\n",
      "[-0.88922001]\n",
      "0.01 [-0.07792345]\n",
      "[-0.96664463]\n",
      "0.01 [-0.0785575]\n",
      "[-0.88844078]\n",
      "0.01 [-0.08091267]\n",
      "[-0.96585905]\n",
      "0.01 [-0.08151912]\n",
      "Iteration 20: x = -0.8876, y = -0.9650, Loss = 0.5964\n",
      "[-0.88763165]\n",
      "0.01 [-0.08379995]\n",
      "[-0.96504386]\n",
      "0.01 [-0.0843696]\n",
      "[-0.88679365]\n",
      "0.01 [-0.08659422]\n",
      "[-0.96420017]\n",
      "0.01 [-0.08711777]\n",
      "[-0.88592771]\n",
      "0.01 [-0.08930311]\n",
      "[-0.96332899]\n",
      "0.01 [-0.0897712]\n",
      "[-0.88503468]\n",
      "0.01 [-0.09193324]\n",
      "[-0.96243128]\n",
      "0.01 [-0.09233642]\n",
      "[-0.88411535]\n",
      "0.01 [-0.09449036]\n",
      "[-0.96150791]\n",
      "0.01 [-0.0948191]\n",
      "[-0.88317044]\n",
      "0.01 [-0.09697953]\n",
      "[-0.96055972]\n",
      "0.01 [-0.0972242]\n",
      "[-0.88220065]\n",
      "0.01 [-0.09940521]\n",
      "[-0.95958748]\n",
      "0.01 [-0.09955607]\n",
      "[-0.88120659]\n",
      "0.01 [-0.10177134]\n",
      "[-0.95859192]\n",
      "0.01 [-0.1018186]\n",
      "[-0.88018888]\n",
      "0.01 [-0.10408146]\n",
      "[-0.95757373]\n",
      "0.01 [-0.10401522]\n",
      "[-0.87914807]\n",
      "0.01 [-0.10633875]\n",
      "[-0.95653358]\n",
      "0.01 [-0.106149]\n",
      "Iteration 30: x = -0.8781, y = -0.9555, Loss = 0.5679\n",
      "[-0.87808468]\n",
      "0.01 [-0.10854604]\n",
      "[-0.95547209]\n",
      "0.01 [-0.10822273]\n",
      "[-0.87699922]\n",
      "0.01 [-0.11070593]\n",
      "[-0.95438986]\n",
      "0.01 [-0.1102389]\n",
      "[-0.87589216]\n",
      "0.01 [-0.11282074]\n",
      "[-0.95328747]\n",
      "0.01 [-0.11219978]\n",
      "[-0.87476395]\n",
      "0.01 [-0.11489262]\n",
      "[-0.95216548]\n",
      "0.01 [-0.11410744]\n",
      "[-0.87361503]\n",
      "0.01 [-0.11692352]\n",
      "[-0.9510244]\n",
      "0.01 [-0.11596376]\n",
      "[-0.87244579]\n",
      "0.01 [-0.11891522]\n",
      "[-0.94986476]\n",
      "0.01 [-0.11777048]\n",
      "[-0.87125664]\n",
      "0.01 [-0.12086938]\n",
      "[-0.94868706]\n",
      "0.01 [-0.1195292]\n",
      "[-0.87004794]\n",
      "0.01 [-0.12278751]\n",
      "[-0.94749177]\n",
      "0.01 [-0.12124137]\n",
      "[-0.86882007]\n",
      "0.01 [-0.12467103]\n",
      "[-0.94627935]\n",
      "0.01 [-0.12290836]\n",
      "[-0.86757336]\n",
      "0.01 [-0.12652122]\n",
      "[-0.94505027]\n",
      "0.01 [-0.12453145]\n",
      "Iteration 40: x = -0.8663, y = -0.9438, Loss = 0.5334\n",
      "[-0.86630815]\n",
      "0.01 [-0.12833931]\n",
      "[-0.94380496]\n",
      "0.01 [-0.1261118]\n",
      "[-0.86502475]\n",
      "0.01 [-0.13012643]\n",
      "[-0.94254384]\n",
      "0.01 [-0.12765052]\n",
      "[-0.86372349]\n",
      "0.01 [-0.13188363]\n",
      "[-0.94126733]\n",
      "0.01 [-0.12914863]\n",
      "[-0.86240465]\n",
      "0.01 [-0.13361189]\n",
      "[-0.93997585]\n",
      "0.01 [-0.1306071]\n",
      "[-0.86106853]\n",
      "0.01 [-0.13531213]\n",
      "[-0.93866978]\n",
      "0.01 [-0.13202684]\n",
      "[-0.85971541]\n",
      "0.01 [-0.13698524]\n",
      "[-0.93734951]\n",
      "0.01 [-0.13340869]\n",
      "[-0.85834556]\n",
      "0.01 [-0.13863201]\n",
      "[-0.93601542]\n",
      "0.01 [-0.13475346]\n",
      "[-0.85695924]\n",
      "0.01 [-0.14025323]\n",
      "[-0.93466789]\n",
      "0.01 [-0.13606191]\n",
      "[-0.85555671]\n",
      "0.01 [-0.14184961]\n",
      "[-0.93330727]\n",
      "0.01 [-0.13733476]\n",
      "[-0.85413821]\n",
      "0.01 [-0.14342184]\n",
      "[-0.93193392]\n",
      "0.01 [-0.1385727]\n",
      "Iteration 50: x = -0.8527, y = -0.9305, Loss = 0.4944\n",
      "[-0.85270399]\n",
      "0.01 [-0.14497056]\n",
      "[-0.93054819]\n",
      "0.01 [-0.13977636]\n",
      "[-0.85125429]\n",
      "0.01 [-0.14649639]\n",
      "[-0.92915043]\n",
      "0.01 [-0.14094637]\n",
      "[-0.84978932]\n",
      "0.01 [-0.14799991]\n",
      "[-0.92774096]\n",
      "0.01 [-0.14208332]\n",
      "[-0.84830933]\n",
      "0.01 [-0.14948166]\n",
      "[-0.92632013]\n",
      "0.01 [-0.14318775]\n",
      "[-0.84681451]\n",
      "0.01 [-0.15094218]\n",
      "[-0.92488825]\n",
      "0.01 [-0.1442602]\n",
      "[-0.84530509]\n",
      "0.01 [-0.15238196]\n",
      "[-0.92344565]\n",
      "0.01 [-0.14530119]\n",
      "[-0.84378127]\n",
      "0.01 [-0.15380147]\n",
      "[-0.92199264]\n",
      "0.01 [-0.1463112]\n",
      "[-0.84224325]\n",
      "0.01 [-0.15520116]\n",
      "[-0.92052953]\n",
      "0.01 [-0.1472907]\n",
      "[-0.84069124]\n",
      "0.01 [-0.15658147]\n",
      "[-0.91905662]\n",
      "0.01 [-0.14824014]\n",
      "[-0.83912543]\n",
      "0.01 [-0.1579428]\n",
      "[-0.91757422]\n",
      "0.01 [-0.14915994]\n",
      "Iteration 60: x = -0.8375, y = -0.9161, Loss = 0.4522\n",
      "[-0.837546]\n",
      "0.01 [-0.15928555]\n",
      "[-0.91608262]\n",
      "0.01 [-0.15005052]\n",
      "[-0.83595314]\n",
      "0.01 [-0.16061008]\n",
      "[-0.91458211]\n",
      "0.01 [-0.15091229]\n",
      "[-0.83434704]\n",
      "0.01 [-0.16191677]\n",
      "[-0.91307299]\n",
      "0.01 [-0.15174561]\n",
      "[-0.83272787]\n",
      "0.01 [-0.16320594]\n",
      "[-0.91155554]\n",
      "0.01 [-0.15255087]\n",
      "[-0.83109581]\n",
      "0.01 [-0.16447793]\n",
      "[-0.91003003]\n",
      "0.01 [-0.15332842]\n",
      "[-0.82945104]\n",
      "0.01 [-0.16573304]\n",
      "[-0.90849674]\n",
      "0.01 [-0.1540786]\n",
      "[-0.8277937]\n",
      "0.01 [-0.16697159]\n",
      "[-0.90695596]\n",
      "0.01 [-0.15480175]\n",
      "[-0.82612399]\n",
      "0.01 [-0.16819386]\n",
      "[-0.90540794]\n",
      "0.01 [-0.1554982]\n",
      "[-0.82444205]\n",
      "0.01 [-0.16940013]\n",
      "[-0.90385296]\n",
      "0.01 [-0.15616824]\n",
      "[-0.82274805]\n",
      "0.01 [-0.17059066]\n",
      "[-0.90229127]\n",
      "0.01 [-0.1568122]\n",
      "Iteration 70: x = -0.8210, y = -0.9007, Loss = 0.4075\n",
      "[-0.82104214]\n",
      "0.01 [-0.1717657]\n",
      "[-0.90072315]\n",
      "0.01 [-0.15743036]\n",
      "[-0.81932449]\n",
      "0.01 [-0.17292551]\n",
      "[-0.89914885]\n",
      "0.01 [-0.15802301]\n",
      "[-0.81759523]\n",
      "0.01 [-0.17407032]\n",
      "[-0.89756862]\n",
      "0.01 [-0.15859042]\n",
      "[-0.81585453]\n",
      "0.01 [-0.17520035]\n",
      "[-0.89598272]\n",
      "0.01 [-0.15913289]\n",
      "[-0.81410252]\n",
      "0.01 [-0.17631582]\n",
      "[-0.89439139]\n",
      "0.01 [-0.15965066]\n",
      "[-0.81233937]\n",
      "0.01 [-0.17741695]\n",
      "[-0.89279488]\n",
      "0.01 [-0.160144]\n",
      "[-0.8105652]\n",
      "0.01 [-0.17850392]\n",
      "[-0.89119344]\n",
      "0.01 [-0.16061316]\n",
      "[-0.80878016]\n",
      "0.01 [-0.17957695]\n",
      "[-0.88958731]\n",
      "0.01 [-0.16105838]\n",
      "[-0.80698439]\n",
      "0.01 [-0.18063622]\n",
      "[-0.88797672]\n",
      "0.01 [-0.16147993]\n",
      "[-0.80517803]\n",
      "0.01 [-0.1816819]\n",
      "[-0.88636193]\n",
      "0.01 [-0.16187802]\n",
      "Iteration 80: x = -0.8034, y = -0.8847, Loss = 0.3611\n",
      "[-0.80336121]\n",
      "0.01 [-0.18271417]\n",
      "[-0.88474314]\n",
      "0.01 [-0.1622529]\n",
      "[-0.80153406]\n",
      "0.01 [-0.18373321]\n",
      "[-0.88312062]\n",
      "0.01 [-0.1626048]\n",
      "[-0.79969673]\n",
      "0.01 [-0.18473916]\n",
      "[-0.88149457]\n",
      "0.01 [-0.16293394]\n",
      "[-0.79784934]\n",
      "0.01 [-0.1857322]\n",
      "[-0.87986523]\n",
      "0.01 [-0.16324054]\n",
      "[-0.79599202]\n",
      "0.01 [-0.18671246]\n",
      "[-0.87823282]\n",
      "0.01 [-0.16352483]\n",
      "[-0.79412489]\n",
      "0.01 [-0.1876801]\n",
      "[-0.87659757]\n",
      "0.01 [-0.16378703]\n",
      "[-0.79224809]\n",
      "0.01 [-0.18863526]\n",
      "[-0.8749597]\n",
      "0.01 [-0.16402734]\n",
      "[-0.79036174]\n",
      "0.01 [-0.18957807]\n",
      "[-0.87331943]\n",
      "0.01 [-0.16424598]\n",
      "[-0.78846596]\n",
      "0.01 [-0.19050866]\n",
      "[-0.87167697]\n",
      "0.01 [-0.16444316]\n",
      "[-0.78656087]\n",
      "0.01 [-0.19142717]\n",
      "[-0.87003254]\n",
      "0.01 [-0.16461909]\n",
      "Iteration 90: x = -0.7846, y = -0.8684, Loss = 0.3136\n",
      "[-0.7846466]\n",
      "0.01 [-0.19233372]\n",
      "[-0.86838635]\n",
      "0.01 [-0.16477397]\n",
      "[-0.78272326]\n",
      "0.01 [-0.19322842]\n",
      "[-0.86673861]\n",
      "0.01 [-0.16490801]\n",
      "[-0.78079098]\n",
      "0.01 [-0.19411139]\n",
      "[-0.86508953]\n",
      "0.01 [-0.16502142]\n",
      "[-0.77884987]\n",
      "0.01 [-0.19498274]\n",
      "[-0.86343931]\n",
      "0.01 [-0.16511439]\n",
      "[-0.77690004]\n",
      "0.01 [-0.19584259]\n",
      "[-0.86178817]\n",
      "0.01 [-0.16518712]\n",
      "[-0.77494161]\n",
      "0.01 [-0.19669103]\n",
      "[-0.8601363]\n",
      "0.01 [-0.16523982]\n",
      "[-0.7729747]\n",
      "0.01 [-0.19752817]\n",
      "[-0.8584839]\n",
      "0.01 [-0.16527269]\n",
      "[-0.77099942]\n",
      "0.01 [-0.1983541]\n",
      "[-0.85683117]\n",
      "0.01 [-0.16528592]\n",
      "[-0.76901588]\n",
      "0.01 [-0.19916893]\n",
      "[-0.85517832]\n",
      "0.01 [-0.16527971]\n",
      "[-0.76702419]\n",
      "0.01 [-0.19997274]\n",
      "[-0.85352552]\n",
      "0.01 [-0.16525427]\n",
      "Iteration 100: x = -0.7650, y = -0.8519, Loss = 0.2657\n",
      "[-0.76502446]\n",
      "0.01 [-0.20076563]\n",
      "[-0.85187298]\n",
      "0.01 [-0.16520979]\n",
      "Optimized Parameters:\n",
      "x1 = -0.7630, x2 = -0.8502\n"
     ]
    }
   ],
   "source": [
    "def solve_constrained_optimization(objective_function, constraints, initial_guess, mu=1.0, lr=1e-2, num_iterations=100):\n",
    "    # Step 1: Extract variables from the objective function\n",
    "    variables = list(f.free_symbols)\n",
    "    variables.sort(key=lambda var: var.name)  # Sort variables for consistency\n",
    "\n",
    "    # Step 2: Create the new objective function with the barrier method\n",
    "    F = create_objective_with_barrier(objective_function, constraints, mu)\n",
    "\n",
    "    # Step 3: Compute the gradient of F\n",
    "    gradient = gradient_function(F, variables)\n",
    "\n",
    "    # Step 4: Convert symbolic expressions to numerical functions\n",
    "    F_numeric = sp.lambdify(variables, F, 'numpy')\n",
    "    gradient_numeric = sp.lambdify(variables, gradient, 'numpy')\n",
    "\n",
    "    # Step 5: Define the objective and gradient functions for RAdam\n",
    "    def objective_function(*params):\n",
    "        return F_numeric(*params)\n",
    "\n",
    "    def gradient_function_wrapper(*params):\n",
    "        grads = gradient_numeric(*params)\n",
    "        return [np.array(grad) for grad in grads]\n",
    "\n",
    "    # Step 6: forming RAdam parameters\n",
    "    params = [np.array([x]) for x in initial_guess]\n",
    "    optimizer = RAdam(lr=lr)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    optimized_params = optimizer.solve(objective_function, params, gradient_function_wrapper, num_iterations)\n",
    "\n",
    "    # Return the optimized parameters\n",
    "    return [p[0] for p in optimized_params]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define symbolic variables\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # User-defined objective function\n",
    "    f = x1**2 + x2**2  # Example: f(x1, x2) = x1^2 + x2^2\n",
    "\n",
    "    # User-defined inequality constraints (g_i(x) <= 0)\n",
    "    constraints = [\n",
    "        x1 + x2 - 1,  # g1(x1, x2) = x1 + x2 - 1 <= 0\n",
    "        -x1 + x2 - 1  # g2(x1, x2) = -x1 + x2 - 1 <= 0\n",
    "    ]\n",
    "\n",
    "    # Initial guess for the variables (!!!the initual guess point must be in feasible region!!!)\n",
    "    initial_guess = [-1.0, -1.0]\n",
    "\n",
    "    # Solve the constrained optimization problem\n",
    "    optimized_params = solve_constrained_optimization(f, constraints, initial_guess, mu=1.0, lr=1e-2, num_iterations=100)\n",
    "\n",
    "    # Print the optimized parameters\n",
    "    print(\"Optimized Parameters:\")\n",
    "    print(f\"x1 = {optimized_params[0]:.4f}, x2 = {optimized_params[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gws2putEVljO"
   },
   "source": [
    "# Forming Tools class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS8jzn1UVotB"
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "class BarrierMethodTools:\n",
    "    \"\"\"A static class containing utility functions for the barrier method.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_barrier_function(constraints):\n",
    "        \"\"\"\n",
    "        Creates the barrier function phi(x) for the given constraints using the property:\n",
    "        sum(log(g_i)) = log(product(g_i)).\n",
    "\n",
    "        Args:\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "\n",
    "        Returns:\n",
    "            sympy expression: The barrier function phi(x).\n",
    "        \"\"\"\n",
    "        # Compute the product of the negative constraints\n",
    "        product_of_constraints = sp.prod(-g for g in constraints)\n",
    "\n",
    "        # Barrier function: phi(x) = -log(product_of_constraints)\n",
    "        phi = -sp.log(product_of_constraints)\n",
    "        return phi\n",
    "\n",
    "    @staticmethod\n",
    "    def create_objective_with_barrier(f, constraints, mu):\n",
    "        \"\"\"\n",
    "        Creates a new objective function with the barrier method applied.\n",
    "\n",
    "        Args:\n",
    "            f (sympy expression): The original objective function.\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "            mu (sympy symbol or float): Barrier parameter.\n",
    "\n",
    "        Returns:\n",
    "            sympy expression: The new objective function F(x, mu).\n",
    "        \"\"\"\n",
    "        # Create the barrier function\n",
    "        phi = BarrierMethodTools.create_barrier_function(constraints)\n",
    "\n",
    "        # New objective function: F(x, mu) = f(x) + mu * phi(x)\n",
    "        F = f + mu * phi\n",
    "        return F\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_function(F, variables):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the function F with respect to the given variables.\n",
    "\n",
    "        Args:\n",
    "            F (sympy expression): The function to compute the gradient for.\n",
    "            variables (list of sympy symbols): List of variables to differentiate with respect to.\n",
    "\n",
    "        Returns:\n",
    "            list of sympy expressions: The gradient of F.\n",
    "        \"\"\"\n",
    "        gradient = [sp.diff(F, var) for var in variables]\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqdM_HJWc21Y",
    "outputId": "63b5ebcf-fb66-4533-fa2d-7e1512fe930b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.]), array([1.])]\n",
      "Iteration 10: x = 0.0000, y = 0.9197, Loss = 0.8460\n",
      "Iteration 20: x = 0.0000, y = 0.9131, Loss = 0.8339\n",
      "Iteration 30: x = 0.0000, y = 0.9036, Loss = 0.8166\n",
      "Iteration 40: x = 0.0000, y = 0.8918, Loss = 0.7954\n",
      "Iteration 50: x = 0.0000, y = 0.8782, Loss = 0.7713\n",
      "Iteration 60: x = 0.0000, y = 0.8630, Loss = 0.7450\n",
      "Iteration 70: x = 0.0000, y = 0.8466, Loss = 0.7168\n",
      "Iteration 80: x = 0.0000, y = 0.8289, Loss = 0.6873\n",
      "Iteration 90: x = 0.0000, y = 0.8103, Loss = 0.6568\n",
      "Iteration 100: x = 0.0000, y = 0.7908, Loss = 0.6256\n",
      "Optimized Parameters:\n",
      "x1 = 0.0000, x2 = 0.7888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# +++TODO #1: adaptive barrier parameter\n",
    "# TODO #2: check the initial guess is in the feasible reagion\n",
    "# TODO #3: error checks\n",
    "def solve_constrained_optimization(f, constraints, initial_guess, mu=1.0, lr=1e-2, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Solve a constrained optimization problem using the barrier method and RAdam optimizer.\n",
    "\n",
    "    Args:\n",
    "        f (sympy expression): The objective function to minimize.\n",
    "        constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "        initial_guess (list of float): Initial guess for the variables.\n",
    "        mu (float): Barrier parameter (default: 1.0).\n",
    "        lr (float): Learning rate for RAdam (default: 1e-2).\n",
    "        num_iterations (int): Number of optimization iterations (default: 100).\n",
    "\n",
    "    Returns:\n",
    "        list of float: Optimized parameters.\n",
    "    \"\"\"\n",
    "    # Extract variables from the objective function only\n",
    "    variables = list(f.free_symbols)\n",
    "    variables.sort(key=lambda var: var.name)  # Sort variables for consistency\n",
    "\n",
    "    # Create the new objective function with the barrier method (symbolic)\n",
    "    F_sym = BarrierMethodTools.create_objective_with_barrier(f, constraints, mu)\n",
    "\n",
    "    # Compute the gradient of F (symbolic)\n",
    "    gradient_sym = BarrierMethodTools.gradient_function(F_sym, variables)\n",
    "\n",
    "    # Convert symbolic expressions to numerical functions\n",
    "    F_numeric = sp.lambdify(variables, F_sym, 'numpy')\n",
    "    gradient_numeric = sp.lambdify(variables, gradient_sym, 'numpy')\n",
    "\n",
    "    # Define the objective and gradient functions for RAdam\n",
    "    def objective_function_wrapper(*params):\n",
    "        return F_numeric(*params)\n",
    "\n",
    "    def gradient_function_wrapper(*params):\n",
    "        grads = gradient_numeric(*params)\n",
    "        return [np.array(grad) for grad in grads]\n",
    "\n",
    "    # Initialize parameters\n",
    "    params = [np.array([x]) for x in initial_guess]\n",
    "    print(params)\n",
    "\n",
    "    # Initialize RAdam optimizer\n",
    "    optimizer = RAdam(lr=lr)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    optimized_params = optimizer.solve(objective_function_wrapper, params, gradient_function_wrapper, num_iterations)\n",
    "\n",
    "    # Return the optimized parameters\n",
    "    return [p[0] for p in optimized_params]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define symbolic variables\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # User-defined objective function\n",
    "    f = x1**2 + x2**2  # Example: f(x1, x2) = x1^2 + x2^2\n",
    "\n",
    "    # User-defined inequality constraints (g_i(x) <= 0)\n",
    "    constraints = [\n",
    "        x1**2-x2\n",
    "        #x1 + x2 - 1,  # g1(x1, x2) = x1 + x2 - 1 <= 0\n",
    "        #-x1 + x2 - 1  # g2(x1, x2) = -x1 + x2 - 1 <= 0\n",
    "    ]\n",
    "\n",
    "    # Initial guess for the variables\n",
    "    initial_guess = [0.0, 1.0]\n",
    "\n",
    "    # Solve the constrained optimization problem\n",
    "    optimized_params = solve_constrained_optimization(f, constraints, initial_guess,\n",
    "                                                      mu=0.001, lr=1e-2, num_iterations=100)\n",
    "\n",
    "    # Print the optimized parameters\n",
    "    print(\"Optimized Parameters:\")\n",
    "    print(f\"x1 = {optimized_params[0]:.4f}, x2 = {optimized_params[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "uFCwmcrtc5q2",
    "outputId": "08f6c48a-806e-482e-99bb-4b73bdcd5491"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.304669$"
      ],
      "text/plain": [
       "1.30466900000000"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.evalf(subs={x1: -0.7630, x2: -0.850})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "HviHPuB0dmzL",
    "outputId": "e8a6144a-d01c-4201-b36d-d9211ff4dddf"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5$"
      ],
      "text/plain": [
       "0.500000000000000"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.evalf(subs={x1: 0.5, x2: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLusGQ0MqC7z"
   },
   "source": [
    "# Adaptive barrier parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C85Lv56TqCt7",
    "outputId": "8a0cfa5c-5272-4e59-e756-e4fb962c52ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Parameters = [0.0, 0.9562350933627074], Loss = 0.9336, mu = 0.3874\n",
      "Iteration 20: Parameters = [0.0, 0.9492441893354145], Loss = 0.9089, mu = 0.1351\n",
      "Iteration 30: Parameters = [0.0, 0.9388080937151324], Loss = 0.8847, mu = 0.0471\n",
      "Iteration 40: Parameters = [0.0, 0.9258240456220205], Loss = 0.8586, mu = 0.0164\n",
      "Iteration 50: Parameters = [0.0, 0.9108814209386893], Loss = 0.8303, mu = 0.0057\n",
      "Iteration 60: Parameters = [0.0, 0.8943843398221092], Loss = 0.8002, mu = 0.0020\n",
      "Iteration 70: Parameters = [0.0, 0.8766101310457759], Loss = 0.7685, mu = 0.0007\n",
      "Iteration 80: Parameters = [0.0, 0.8577562971042157], Loss = 0.7358, mu = 0.0002\n",
      "Iteration 90: Parameters = [0.0, 0.8379730793690617], Loss = 0.7022, mu = 0.0001\n",
      "Iteration 100: Parameters = [0.0, 0.8173831550179095], Loss = 0.6681, mu = 0.0000\n",
      "Optimized Parameters:\n",
      "x1 = 0.0000, x2 = 0.8153\n"
     ]
    }
   ],
   "source": [
    "# as mu approaches zero, the closer to the original function we\n",
    "def solve_constrained_optimization(f, constraints, initial_guess, mu_0=1.0, lr=1e-2, num_iterations=100, mu_decay=0.9):\n",
    "    \"\"\"\n",
    "    Solve a constrained optimization problem using the barrier method and RAdam optimizer.\n",
    "\n",
    "    Args:\n",
    "        f (sympy expression): The objective function to minimize.\n",
    "        constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "        initial_guess (list or np.ndarray): Initial guess for the variables.\n",
    "        mu_0 (float): Initial barrier parameter (default: 1.0).\n",
    "        lr (float): Learning rate for RAdam (default: 1e-2).\n",
    "        num_iterations (int): Number of optimization iterations (default: 100).\n",
    "        mu_decay (float): Decay factor for the barrier parameter (default: 0.9).\n",
    "\n",
    "    Returns:\n",
    "        list of float: Optimized parameters.\n",
    "    \"\"\"\n",
    "    # Extract variables from the objective function only\n",
    "    variables = list(f.free_symbols)\n",
    "    variables.sort(key=lambda var: var.name)  # Sort variables for consistency\n",
    "\n",
    "    # Initialize the barrier parameter\n",
    "    mu = mu_0\n",
    "\n",
    "    # Initialize RAdam optimizer\n",
    "    optimizer = RAdam(lr=lr)\n",
    "\n",
    "    # Convert initial_guess to a list of np.array objects\n",
    "    params = [np.array([x]) for x in initial_guess]\n",
    "\n",
    "    # Optimization loop\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        # Create the new objective function with the current barrier parameter\n",
    "        F_sym = BarrierMethodTools.create_objective_with_barrier(f, constraints, mu)\n",
    "\n",
    "        # Compute the gradient of F (symbolic)\n",
    "        gradient_sym = BarrierMethodTools.gradient_function(F_sym, variables)\n",
    "\n",
    "        # Convert symbolic expressions to numerical functions\n",
    "        F_numeric = sp.lambdify(variables, F_sym, 'numpy')\n",
    "        gradient_numeric = sp.lambdify(variables, gradient_sym, 'numpy')\n",
    "\n",
    "        # Define the objective and gradient functions for RAdam\n",
    "        def objective_function(*params):\n",
    "            return F_numeric(*params)\n",
    "\n",
    "        def gradient_function_wrapper(*params):\n",
    "            grads = gradient_numeric(*params)\n",
    "            return [np.array(grad) for grad in grads]\n",
    "\n",
    "        # Perform one optimization step\n",
    "        params = optimizer.step(params, gradient_function_wrapper(*params), iteration)\n",
    "\n",
    "        # Update the barrier parameter\n",
    "        mu *= mu_decay\n",
    "\n",
    "        # Print progress\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            loss = objective_function(*params)\n",
    "            print(f\"Iteration {iteration + 1}: Parameters = {[p[0] for p in params]}, Loss = {loss[0]:.4f}, mu = {mu:.4f}\")\n",
    "\n",
    "    # Return the optimized parameters as a list of floats\n",
    "    return [p[0] for p in params]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define symbolic variables\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # User-defined objective function\n",
    "    f = x1**2 + x2**2  # Example: f(x1, x2) = x1^2 + x2^2\n",
    "\n",
    "    # User-defined inequality constraints (g_i(x) <= 0)\n",
    "    constraints = [\n",
    "        x1**2-x2\n",
    "        #x1 + x2 - 1,  # g1(x1, x2) = x1 + x2 - 1 <= 0\n",
    "        #-x1 + x2 - 1  # g2(x1, x2) = -x1 + x2 - 1 <= 0\n",
    "    ]\n",
    "\n",
    "    # Initial guess for the variables\n",
    "    initial_guess = [0.0, 1.0]\n",
    "\n",
    "    # Solve the constrained optimization problem\n",
    "    optimized_params = solve_constrained_optimization(\n",
    "        f, constraints, initial_guess, mu_0=1.0, lr=1e-2, num_iterations=100, mu_decay=0.9\n",
    "    )\n",
    "\n",
    "    # Print the optimized parameters\n",
    "    print(\"Optimized Parameters:\")\n",
    "    print(f\"x1 = {optimized_params[0]:.4f}, x2 = {optimized_params[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cfRlZ5tqKDt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUHCfj4byERX"
   },
   "source": [
    "# Adaptive barrier method as black-box interface to Barrier Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOicXNFvyK-l",
    "outputId": "4d8a686d-d69d-4873-b2ac-4a35c8067c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Parameters = [-1.80063317411976, -4.645952343098949], Loss = 24.6723, mu = 0.3874\n",
      "Iteration 20: Parameters = [-1.7944646894826297, -4.639265593251782], Loss = 24.6915, mu = 0.1351\n",
      "Iteration 30: Parameters = [-1.7850249910943359, -4.629494133963085], Loss = 24.6021, mu = 0.0471\n",
      "Iteration 40: Parameters = [-1.7730121586729897, -4.6173891781333465], Loss = 24.4588, mu = 0.0164\n",
      "Iteration 50: Parameters = [-1.7589248413038039, -4.603360554067481], Loss = 24.2833, mu = 0.0057\n",
      "Iteration 60: Parameters = [-1.743124934694012, -4.587675802629339], Loss = 24.0849, mu = 0.0020\n",
      "Iteration 70: Parameters = [-1.725870723142038, -4.570523561563857], Loss = 23.8683, mu = 0.0007\n",
      "Iteration 80: Parameters = [-1.7073490202925448, -4.552044616899687], Loss = 23.6362, mu = 0.0002\n",
      "Iteration 90: Parameters = [-1.6877004213556264, -4.532349377029768], Loss = 23.3905, mu = 0.0001\n",
      "Iteration 100: Parameters = [-1.667036273292761, -4.51152808033489], Loss = 23.1329, mu = 0.0000\n",
      "Optimized Parameters:\n",
      "x1 = -1.6649, x2 = -4.5094\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "class BarrierMethodTools:\n",
    "    \"\"\"A static class containing utility functions for the barrier method.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_barrier_function(constraints, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Creates the barrier function phi(x) for the given constraints using the logarithm of the product.\n",
    "\n",
    "        Args:\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "            epsilon (float): Small offset to ensure constraints are strictly less than zero.\n",
    "\n",
    "        Returns:\n",
    "            sympy expression: The barrier function phi(x).\n",
    "        \"\"\"\n",
    "        # Add a small offset to the constraints to avoid g_i(x) = 0\n",
    "        offset_constraints = [g - epsilon for g in constraints]\n",
    "\n",
    "        # Barrier function: phi(x) = -log(product_of_constraints)\n",
    "        product_of_constraints = sp.prod(-g for g in offset_constraints)\n",
    "        phi = -sp.log(product_of_constraints)\n",
    "        return phi\n",
    "\n",
    "    @staticmethod\n",
    "    def create_objective_with_barrier(f, constraints, mu):\n",
    "        \"\"\"\n",
    "        Creates a new objective function with the barrier method applied.\n",
    "\n",
    "        Args:\n",
    "            f (sympy expression): The original objective function.\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "            mu (sympy symbol or float): Barrier parameter.\n",
    "\n",
    "        Returns:\n",
    "            sympy expression: The new objective function F(x, mu).\n",
    "        \"\"\"\n",
    "        # Create the barrier function\n",
    "        phi = BarrierMethodTools.create_barrier_function(constraints)\n",
    "\n",
    "        # New objective function: F(x, mu) = f(x) + mu * phi(x)\n",
    "        F = f + mu * phi\n",
    "        return F\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_function(F, variables):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the function F with respect to the given variables.\n",
    "\n",
    "        Args:\n",
    "            F (sympy expression): The function to compute the gradient for.\n",
    "            variables (list of sympy symbols): List of variables to differentiate with respect to.\n",
    "\n",
    "        Returns:\n",
    "            list of sympy expressions: The gradient of F.\n",
    "        \"\"\"\n",
    "        gradient = [sp.diff(F, var) for var in variables]\n",
    "        return gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def is_feasible(constraints, variables, point, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Check if a point is feasible (satisfies all constraints).\n",
    "\n",
    "        Args:\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "            variables (list of sympy symbols): List of variables in the constraints.\n",
    "            point (list or np.ndarray): The point to check.\n",
    "            epsilon (float): Tolerance for feasibility (default: 1e-6).\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the point is feasible, False otherwise.\n",
    "        \"\"\"\n",
    "        # Convert constraints to numerical functions\n",
    "        constraint_funcs = [sp.lambdify(variables, g, 'numpy') for g in constraints]\n",
    "\n",
    "        # Evaluate constraints at the point\n",
    "        for g_func in constraint_funcs:\n",
    "            if g_func(*point) >= -epsilon:\n",
    "                return False  # Constraint violated\n",
    "\n",
    "        return True  # All constraints satisfied\n",
    "\n",
    "    @staticmethod\n",
    "    def find_feasible_point(constraints, variables, initial_guess, optimizer, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Find a feasible point by minimizing the constraint violations.\n",
    "\n",
    "        Args:\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "            variables (list of sympy symbols): List of variables in the constraints.\n",
    "            initial_guess (list or np.ndarray): Initial guess for the variables.\n",
    "            optimizer: The optimizer to use (e.g., RAdam).\n",
    "            max_iterations (int): Maximum number of iterations to find a feasible point (default: 1000).\n",
    "\n",
    "        Returns:\n",
    "            list of float: A feasible point.\n",
    "        \"\"\"\n",
    "        # Define the objective function as the sum of squared constraint violations\n",
    "        violation = sum(sp.Max(0, g)**2 for g in constraints)\n",
    "        violation_func = sp.lambdify(variables, violation, 'numpy')\n",
    "\n",
    "        # Define the gradient of the violation function\n",
    "        gradient_violation = [sp.diff(violation, var) for var in variables]\n",
    "        gradient_violation_func = sp.lambdify(variables, gradient_violation, 'numpy')\n",
    "\n",
    "        # Convert initial_guess to a list of np.array objects\n",
    "        params = [np.array([x]) for x in initial_guess]\n",
    "\n",
    "        # Minimize the violation function\n",
    "        for iteration in range(1, max_iterations + 1):\n",
    "            # Extract scalar values from params\n",
    "            param_values = [p[0] for p in params]\n",
    "\n",
    "            # Compute gradients\n",
    "            grads = np.array(gradient_violation_func(*param_values)).reshape(-1, 1)  # Ensure grads is a single array\n",
    "\n",
    "            # Perform optimization step\n",
    "            params = optimizer.step(params, grads, iteration)\n",
    "\n",
    "            # Check if the point is feasible\n",
    "            if BarrierMethodTools.is_feasible(constraints, variables, [p[0] for p in params]):\n",
    "                return [p[0] for p in params]\n",
    "\n",
    "        raise ValueError(\"Unable to find a feasible point within the maximum number of iterations.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize_with_adaptive_barrier(f, constraints, initial_guess, optimizer, mu_0=1.0, num_iterations=100, mu_decay=0.9):\n",
    "        \"\"\"\n",
    "        Solve a constrained optimization problem using the barrier method and a given optimizer.\n",
    "\n",
    "        Args:\n",
    "            f (sympy expression): The objective function to minimize.\n",
    "            constraints (list of sympy expressions): List of inequality constraints (g_i(x) <= 0).\n",
    "            initial_guess (list of floats or np.ndarray of floats): Initial guess for the variables.\n",
    "            optimizer: The optimizer to use (e.g., RAdam).\n",
    "            mu_0 (float): Initial barrier parameter (default: 1.0).\n",
    "            num_iterations (int): Number of optimization iterations (default: 100).\n",
    "            mu_decay (float): Decay factor for the barrier parameter (default: 0.9).\n",
    "\n",
    "        Returns:\n",
    "            list of float: Optimized parameters.\n",
    "        \"\"\"\n",
    "        # Extract variables from the objective function only\n",
    "        variables = list(f.free_symbols)\n",
    "        variables.sort(key=lambda var: var.name)  # Sort variables for consistency\n",
    "\n",
    "        # Check if the initial guess is feasible\n",
    "        if not BarrierMethodTools.is_feasible(constraints, variables, initial_guess):\n",
    "            raise ValueError(\"Initial guess is not in the feasible set!\")\n",
    "\n",
    "        # Initialize the barrier parameter\n",
    "        mu = mu_0\n",
    "\n",
    "        # Convert initial_guess to a list of np.array objects\n",
    "        params = [np.array([x]) for x in initial_guess]\n",
    "\n",
    "        # Optimization loop\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            # Create the new objective function with the current barrier parameter\n",
    "            F_sym = BarrierMethodTools.create_objective_with_barrier(f, constraints, mu)\n",
    "\n",
    "            # Compute the gradient of F (symbolic)\n",
    "            gradient_sym = BarrierMethodTools.gradient_function(F_sym, variables)\n",
    "\n",
    "            # Convert symbolic expressions to numerical functions\n",
    "            F_numeric = sp.lambdify(variables, F_sym, 'numpy')\n",
    "            gradient_numeric = sp.lambdify(variables, gradient_sym, 'numpy')\n",
    "\n",
    "            # Define the objective and gradient functions for the optimizer\n",
    "            def objective_function(*params):\n",
    "                return F_numeric(*params)\n",
    "\n",
    "            def gradient_function_wrapper(*params):\n",
    "                grads = gradient_numeric(*params)\n",
    "                return [np.array(grad) for grad in grads]\n",
    "\n",
    "            # Perform one optimization step\n",
    "            params = optimizer.step(params, gradient_function_wrapper(*params), iteration)\n",
    "\n",
    "            # Update the barrier parameter\n",
    "            mu *= mu_decay\n",
    "\n",
    "            # Print progress\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                loss = objective_function(*params)\n",
    "                print(f\"Iteration {iteration + 1}: Parameters = {[p[0] for p in params]}, Loss = {loss[0]:.4f}, mu = {mu:.4f}\")\n",
    "\n",
    "        # Return the optimized parameters as a list of floats\n",
    "        return [p[0] for p in params]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define symbolic variables\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # User-defined objective function\n",
    "    f = x1**2 + x2**2  # Example: f(x1, x2) = x1^2 + x2^2\n",
    "\n",
    "    # User-defined inequality constraints (g_i(x) <= 0)\n",
    "    constraints = [\n",
    "        x1**2 + x2,\n",
    "        sp.exp(x1) + x2,\n",
    "        sp.cos(x1)\n",
    "        #x1 + x2 - 1,  # g1(x1, x2) = x1 + x2 - 1 <= 0\n",
    "        #-x1 + x2 - 1  # g2(x1, x2) = -x1 + x2 - 1 <= 0\n",
    "    ]\n",
    "\n",
    "    # Initial guess for the variables\n",
    "    initial_guess = [-2.0, -5.0]\n",
    "\n",
    "    # Initialize RAdam optimizer\n",
    "    optimizer = RAdam(lr=1e-2)\n",
    "\n",
    "    # Solve the constrained optimization problem using BarrierMethodTools\n",
    "    optimized_params = BarrierMethodTools.optimize_with_adaptive_barrier(\n",
    "        f, constraints, initial_guess, optimizer, mu_0=1.0, num_iterations=100, mu_decay=0.9\n",
    "    )\n",
    "\n",
    "    # Print the optimized parameters\n",
    "    print(\"Optimized Parameters:\")\n",
    "    print(f\"x1 = {optimized_params[0]:.4f}, x2 = {optimized_params[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KSakyQVRLWQ6",
    "UCs4XU6mL8Dd",
    "D0T9gAK3MAPM",
    "hh-VoOuTNANy",
    "N_PgHngoP9T3",
    "gws2putEVljO",
    "CLusGQ0MqC7z"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
